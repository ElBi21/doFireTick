{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import os\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# Custom Dataset to include original, flipped, and conditionally augmented flipped images\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, dataset, additional_transforms=None, augment_prob=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: The original dataset (e.g., ImageFolder).\n",
    "            additional_transforms: Additional transformations to apply.\n",
    "            augment_prob: Probability of applying secondary transformations to flipped images.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.additional_transforms = additional_transforms\n",
    "        self.augment_prob = augment_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        # Double the dataset size (original + flipped)\n",
    "        return len(self.dataset) * 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx % len(self.dataset)  # Get the original image index\n",
    "        is_flipped = idx >= len(self.dataset)  # Check if it's a flipped image\n",
    "\n",
    "        image, label = self.dataset[original_idx]\n",
    "\n",
    "        # Apply horizontal flip if needed\n",
    "        if is_flipped:\n",
    "            image = transforms.functional.hflip(image)\n",
    "\n",
    "            # Randomly apply secondary transformations\n",
    "            if random.random() < self.augment_prob and self.additional_transforms:\n",
    "                image = self.additional_transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform_original = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define additional transformations for flipped images\n",
    "additional_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=15),  # Random rotation\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    transforms.Lambda(lambda img: torch.clamp(img + torch.randn_like(img) * 0.07, 0.0, 1.0))  # Brightness noise\n",
    "])\n",
    "\n",
    "# Load original dataset\n",
    "all_data_path = r\"C:\\Users\\repea\\Downloads\\dl2425_challenge_dataset\\dl2425_challenge_dataset\\train\"\n",
    "#all_data_path = \"/kaggle/input/fire-not-fire/dl2425_challenge_dataset/train\"\n",
    "original_dataset = datasets.ImageFolder(root=all_data_path, transform=transform_original)\n",
    "\n",
    "# Create the augmented dataset\n",
    "augmented_dataset = AugmentedDataset(\n",
    "    original_dataset,\n",
    "    additional_transforms=additional_transforms,\n",
    "    augment_prob=0.3  # 30% of flipped images will have secondary transformations\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(augmented_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Verify the DataLoader\n",
    "print(f\"Number of training samples (including flipped): {len(train_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both datasets into one for splitting\n",
    "all_data_path = r\"C:\\Users\\repea\\Downloads\\dl2425_challenge_dataset\\dl2425_challenge_dataset\\val\"\n",
    "categories = [\"0\", \"1\"]\n",
    "\n",
    "# Define transformations (resize to 224x224 and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Convert image to PyTorch tensor\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(root=all_data_path, transform=transform)\n",
    "\n",
    "\n",
    "# Create DataLoader for train and test sets\n",
    "val_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Verify the dataloaders\n",
    "print(f\"Number of training samples: {len(val_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in val_loader:\n",
    "    # Plot the first image in the batch\n",
    "    image_tensor = batch[0][0]\n",
    "    image = image_tensor.permute(1, 2, 0).numpy()  # Convert to H x W x C for plotting\n",
    "    plt.imshow(image)\n",
    "    plt.grid(True)\n",
    "    plt.axis('on')\n",
    "    plt.show()\n",
    "    print (image_tensor.shape, batch[1][0])\n",
    "    break  # Display only one image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super().__init__()\n",
    "        # use standard CE loss without reducion as basis\n",
    "        self.CE = nn.CrossEntropyLoss(reduction='none',)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        '''\n",
    "        input (B, N)\n",
    "        target (B)\n",
    "        '''\n",
    "        CE_loss = self.CE(input, target)\n",
    "        pt = torch.exp(-CE_loss) # don't forget the minus here\n",
    "        focal_loss = (1-pt)**self.gamma * CE_loss\n",
    "\n",
    "        # apply class weights\n",
    "        if self.alpha != None:\n",
    "            focal_loss *= self.alpha.gather(0, target)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            focal_loss = focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            focal_loss = focal_loss.sum()\n",
    "        return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = torchvision.models.resnet152(pretrained=True).to(device)\n",
    "resnet.fc = nn.Linear(2048,1024)\n",
    "\n",
    "class lll(nn.Module):\n",
    "    \"\"\"Some Information about MyModule\"\"\"\n",
    "    def __init__(self, backbone):\n",
    "        super(lll, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.linear = nn.Linear(1024,512)\n",
    "        self.linear2 = nn.Linear(512,2)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.backbone(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.linear(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n",
    "    \n",
    "model1= lll(resnet).to(device)\n",
    "optimizer1 = optim.AdamW(model1.parameters(), lr=0.001)\n",
    "criterion1 = FocalLoss()\n",
    "###a complex 152 resnet with changed last layer and added subsequent layers, trained with cross entropy loss anf AdamW with decreasing lr rate, data augmentation applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\").to(device)\n",
    "model2.fc = nn.Linear(2048,2).to(device)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.1)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "\n",
    "#HAVE TO COMMENT THE DATA AUGMENTATION PART TO TRAIN THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = torchvision.models.resnet18(weights=\"ResNet18_Weights.DEFAULT\").to(device)\n",
    "model3.fc = nn.Linear(512,2).to(device)\n",
    "optimizer3 = optim.SGD(model3.parameters(), lr=0.1)\n",
    "criterion3 = FocalLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            inputs = rgb2stef(inputs.permute(0,2,3,1)).permute(0,3,1,2)\n",
    "            outputs = model(inputs.to(device))  # Forward pass\n",
    "            loss = criterion(outputs, labels.to(device))  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update weights\n",
    "            running_loss += loss.item()\n",
    "            print (\"ok\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "\n",
    "train_model(model2,train_loader_model2, optimizer, criterion_model2, num_epochs=10)\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = g['lr']*0.1\n",
    "train_model(model2,train_loader_model2, optimizer, criterion_model2, num_epochs=10)\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = g['lr']*0.1\n",
    "train_model(model2,train_loader_model2, optimizer, criterion_model2, num_epochs=10)\n",
    "for g in optimizer1.param_groups:\n",
    "    g['lr'] = g['lr']*0.1\n",
    "train_model(model2,train_loader_model2, optimizer, criterion_model2, num_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnsembleModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, model1, model2, model3, mode=\"average\"):\n",
    "        \"\"\"\n",
    "        Ensemble model that combines predictions from models.\n",
    "\n",
    "        Args:\n",
    "            model1: First model.\n",
    "            model2: Second model.\n",
    "            model3: Third model.\n",
    "            mode: How to combine predictions ('average', 'weighted', 'vote').\n",
    "        \"\"\"\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "        self.mode = mode\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get predictions (logits) from the models\n",
    "        output1 = self.model1(x)\n",
    "        output2 = self.model2(x)\n",
    "        output3 = self.model3(x)\n",
    "\n",
    "        if self.mode == \"average\":\n",
    "            # Combine outputs using simple averaging\n",
    "            return (output1 + output2 + output3) / 3\n",
    "        elif self.mode == \"weighted\":\n",
    "            # Example of weighted averaging\n",
    "            w1, w2, w3 = 0.5, 0.3, 0.2  # Example weights\n",
    "            return w1 * output1 + w2 * output2 + w3 * output3\n",
    "        elif self.mode == \"vote\":\n",
    "            # Majority voting\n",
    "            # Get predicted class probabilities from each model using softmax\n",
    "            prob1 = F.softmax(output1, dim=1)\n",
    "            prob2 = F.softmax(output2, dim=1)\n",
    "            prob3 = F.softmax(output3, dim=1)\n",
    "\n",
    "            # Combine probabilities by voting (summing probabilities for each class)\n",
    "            combined_prob = prob1 + prob2 + prob3\n",
    "\n",
    "            return combined_prob  # Return the combined probabilities\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported combination mode!\")\n",
    "\n",
    "        \n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models to device\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "model3.to(device)\n",
    "\n",
    "# Set models to evaluation mode (important for inference)\n",
    "model1.eval()\n",
    "model2.eval()\n",
    "model3.eval()\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble_model = EnsembleModel(model1, model2, model3, mode=\"average\")\n",
    "ensemble_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs.to(device))\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "        print(f\"Test Accuracy: {100 * correct / total}%\")\n",
    "\n",
    "test_model(ensemble_model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
